# numc

Here's what I did in project 4:

In numc.c, I implemented several number methods that can be used to manipulate numc.Matrix objects more conveniently and quicker than a simple Python array could allow. These methods are addition (A+B), subtraction (A-B), multiplication (AB), exponentiation (A^5), negation (-A), and absolute value (abs(A)). In addition, I completed the get and set methods of numc.Matrix objects, so that values in matrices can be interacted with (A.get(2, 3), A.set(5, 1, 2.63), A[63, 23] = 352.35).

I found the most difficult part of implementing these was figuring out how to work with PyObjects and other structures in the Py-C interface - once that was figured out it was just a matter of calling my matrix.c functions, which involved largely straightforward looping through all of the matrices' elements. It required a lot of reading the API reference manual, and even after that I found myself often having to debug. Handling exceptions was surprisingly easy, though. I realistically expect some of the hidden tests will catch errors in memory leaks, but at this point I'm hoping to get lucky. I at least know slices will keep their data even if their parent is no longer referenced.

Getting the speedup was rather complicated. I opted to use unrolling and limit function calls - though I suspect the SIMD instructions made that a somewhat futile change. The SIMD instructions were a bit of a nightmare - it took me a minute to realize how to increment memory addresses correctly. For multiplication, it became apparent that for both cache optimization and SIMD purposes it was ideal to make a transpose matrix of the second argument. That became a headache in terms of getting the indexing right again. Finally, one of the most significant performance boosters whas using OpenMP - in my implementation, this was pretty easy. A couple minor adjustments were made to keep the right variables private between threads, but parallelizing the outermost loops in my functions involved surprisingly few conflicts.

Getting the exponentiation speedup right was a bit tricky - it was already significantly faster than the naive solution before I made the transpose change in multiply, so I was hoping the transpose change in multiply would bring exponentiation all the way up to speed. However, it wasn't quite significant enough, so I had to play around with the pattern of multiplication; by squaring the matrix a couple of times and then repeatedly multiplying by itself rather than just performing pow multiplies, I achieved a much better result.

One of the biggest takeaways for me is that there's a pretty significant cost to optimizing for large data - it's a lot of extra logic for parallelization and for handling tail cases when unrolling, using SIMD, or similar operations. Nonetheless, the cost of taking a couple orders of magnitude longer to perform operations on small data is significantly outweighed by a decent degree of speedup for larger data. The small tests could get 1000x slower and it would not effect the runtime of the testing nearly as much as a 5x speedup for matrices of a significant enough size.
